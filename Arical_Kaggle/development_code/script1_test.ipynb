{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading data from disk ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/Arical_Kaggle/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (22,32,34,49,55) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing data for LightGBM ...\n",
      "(90275, 53) (90275,)\n",
      "\n",
      "Fitting LightGBM model ...\n",
      "\n",
      "Prepare for LightGBM prediction ...\n",
      "   Read sample file ...\n",
      "   ...\n",
      "   Merge with property data ...\n",
      "   ...\n",
      "   ...\n",
      "   ...\n",
      "   Preparing x_test...\n",
      "   ...\n",
      "\n",
      "Start LightGBM prediction ...\n",
      "\n",
      "Unadjusted LightGBM predictions:\n",
      "          0\n",
      "0  0.035491\n",
      "1  0.038709\n",
      "2  0.009419\n",
      "3  0.007204\n",
      "4  0.008871\n",
      "\n",
      "Re-reading properties file ...\n",
      "\n",
      "Processing data for XGBoost ...\n",
      "Shape train: (90275, 57)\n",
      "Shape test: (2985217, 57)\n",
      "After removing outliers:\n",
      "Shape train: (88525, 57)\n",
      "Shape test: (2985217, 57)\n",
      "\n",
      "Setting up data for XGBoost ...\n",
      "\n",
      "XGBoost tuned with CV in:\n",
      "   https://www.kaggle.com/aharless/xgboost-without-outliers-tweak \n",
      "num_boost_rounds=242\n",
      "\n",
      "Training XGBoost ...\n",
      "\n",
      "Predicting with XGBoost ...\n",
      "\n",
      "XGBoost predictions:\n",
      "          0\n",
      "0 -0.027490\n",
      "1 -0.022037\n",
      "2  0.044982\n",
      "3  0.071141\n",
      "4  0.008121\n",
      "\n",
      "Combining XGBoost, LightGBM, and baseline predicitons ...\n",
      "\n",
      "Combined predictions:\n",
      "          0\n",
      "0 -0.005581\n",
      "1 -0.000928\n",
      "2  0.032546\n",
      "3  0.048787\n",
      "4  0.008398\n",
      "\n",
      "Preparing results for write ...\n",
      "\n",
      "Writing results to disk ...\n",
      "\n",
      "Finished ...\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "XGB_WEIGHT = 0.6500\n",
    "BASELINE_WEIGHT = 0.0056\n",
    "\n",
    "BASELINE_PRED = 0.0115\n",
    "\n",
    "# version 61\n",
    "#   Drop fireplacecnt and fireplaceflag, following Jayaraman:\n",
    "#     https://www.kaggle.com/valadi/xgb-w-o-outliers-lgb-with-outliers-combo-tune5\n",
    "\n",
    "# version 60\n",
    "#   Try BASELINE_PRED=0.0115, since that's the actual baseline from\n",
    "#     https://www.kaggle.com/aharless/oleg-s-original-better-baseline\n",
    "\n",
    "# version 59\n",
    "#   Looks like 0.0056 is the optimum BASELINE_WEIGHT\n",
    "\n",
    "# versions 57, 58\n",
    "#   Playing with BASELINE_WEIGHT parameter:\n",
    "#     3 values will determine quadratic approximation of optimum\n",
    "\n",
    "# version 55\n",
    "#   OK, it doesn't get the same result, but I also get a different result\n",
    "#     if I fork the earlier version and run it again.\n",
    "#   So something weird is going on (maybe software upgrade??)\n",
    "#   I'm just going to submit this version and make it my new benchmark.\n",
    "\n",
    "# version 53\n",
    "#   Re-parameterize ensemble (should get same result).\n",
    "\n",
    "# version 51\n",
    "#   Quadratic approximation based on last 3 submissions gives 0.3533\n",
    "#     as optimal lgb_weight.  To be slightly conservative,\n",
    "#     I'm rounding down to 0.35\n",
    "\n",
    "# version 50\n",
    "#   Quadratic approximation based on last 3 submissions gives 0.3073 \n",
    "#     as optimal lgb_weight\n",
    "\n",
    "# version 49\n",
    "#   My latest quadratic approximation is concave, so I'm just taking\n",
    "#     a shot in the dark with lgb_weight=.3\n",
    "\n",
    "# version 45\n",
    "#   Increase lgb_weight to 0.25 based on new quadratic approximation.\n",
    "#   Based on scores for versions 41, 43, and 44, the optimum is 0.261\n",
    "#     if I've done the calculations right.\n",
    "#   I'm being conservative and only going 2/3 of the way there.\n",
    "#   (FWIW my best guess is that even this will get a worse score,\n",
    "#    but you gotta pay some attention to the math.)\n",
    "\n",
    "# version 44\n",
    "#   Increase lgb_weight to 0.23, per Nikunj's suggestion, even though\n",
    "#     my quadratic approximation said I was already at the optimum\n",
    "\n",
    "# verison 43\n",
    "#   Higher lgb_weight, so I can do a quadratic approximation of the optimum\n",
    "\n",
    "# version 42\n",
    "#   The answer to the ultimate question of life, the universe, and everything\n",
    "#     comes down to a slightly higher lgb_weight\n",
    "\n",
    "# version 41\n",
    "#   Trying Nikunj's suggestion of imputing missing values.\n",
    "\n",
    "# version 39\n",
    "#   Trying higher lgb_weight again but with old learning rate.\n",
    "#   The new one did better with LGB only but makes the combination worse.\n",
    "\n",
    "# version 38\n",
    "#   OK back to baseline 0.2 weight\n",
    "\n",
    "# version 37\n",
    "#   Looks like increasing lgb_weight was better\n",
    "\n",
    "# version 34\n",
    "#   OK, try reducing lgb_weight instead\n",
    "\n",
    "# version 32\n",
    "#   Increase lgb_weight because LGB performance has imporved more than XGB\n",
    "#   Increase learning rate for LGB: 0029 is compromise;  CV prefers 0033\n",
    "#     (and reallly would prefer more boosting rounds with old value instead\n",
    "#      but constaints on running time are getting hard)\n",
    "\n",
    "# Version 27:\n",
    "#   Control LightGBM's loquacity\n",
    "\n",
    "# Version 26:\n",
    "# Getting rid of the LightGBM validation, since this script doesn't use the result.\n",
    "# Now use all training data to fit model.\n",
    "# I have a separate script for validation:\n",
    "#    https://www.kaggle.com/aharless/lightgbm-outliers-remaining-cv\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "\n",
    "\n",
    "##### READ IN RAW DATA\n",
    "\n",
    "print( \"\\nReading data from disk ...\")\n",
    "prop = pd.read_csv('../input/properties_2016.csv')\n",
    "train = pd.read_csv(\"../input/train_2016_v2.csv\")\n",
    "\n",
    "\n",
    "\n",
    "##### PROCESS DATA FOR LIGHTGBM\n",
    "\n",
    "print( \"\\nProcessing data for LightGBM ...\" )\n",
    "for c, dtype in zip(prop.columns, prop.dtypes):\t\n",
    "    if dtype == np.float64:\t\t\n",
    "        prop[c] = prop[c].astype(np.float32)\n",
    "\n",
    "df_train = train.merge(prop, how='left', on='parcelid')\n",
    "df_train.fillna(df_train.median(),inplace = True)\n",
    "\n",
    "x_train = df_train.drop(['parcelid', 'logerror', 'transactiondate', 'propertyzoningdesc', \n",
    "                         'propertycountylandusecode', 'fireplacecnt', 'fireplaceflag'], axis=1)\n",
    "y_train = df_train['logerror'].values\n",
    "print(x_train.shape, y_train.shape)\n",
    "\n",
    "\n",
    "train_columns = x_train.columns\n",
    "\n",
    "for c in x_train.dtypes[x_train.dtypes == object].index.values:\n",
    "    x_train[c] = (x_train[c] == True)\n",
    "\n",
    "del df_train; gc.collect()\n",
    "\n",
    "x_train = x_train.values.astype(np.float32, copy=False)\n",
    "d_train = lgb.Dataset(x_train, label=y_train)\n",
    "\n",
    "\n",
    "\n",
    "##### RUN LIGHTGBM\n",
    "\n",
    "params = {}\n",
    "params['max_bin'] = 10\n",
    "params['learning_rate'] = 0.0021 # shrinkage_rate\n",
    "params['boosting_type'] = 'gbdt'\n",
    "params['objective'] = 'regression'\n",
    "params['metric'] = 'l1'          # or 'mae'\n",
    "params['sub_feature'] = 0.5      # feature_fraction -- OK, back to .5, but maybe later increase this\n",
    "params['bagging_fraction'] = 0.85 # sub_row\n",
    "params['bagging_freq'] = 40\n",
    "params['num_leaves'] = 512        # num_leaf\n",
    "params['min_data'] = 500         # min_data_in_leaf\n",
    "params['min_hessian'] = 0.05     # min_sum_hessian_in_leaf\n",
    "params['verbose'] = 0\n",
    "\n",
    "print(\"\\nFitting LightGBM model ...\")\n",
    "clf = lgb.train(params, d_train, 430)\n",
    "\n",
    "del d_train; gc.collect()\n",
    "del x_train; gc.collect()\n",
    "\n",
    "print(\"\\nPrepare for LightGBM prediction ...\")\n",
    "print(\"   Read sample file ...\")\n",
    "sample = pd.read_csv('../input/sample_submission.csv')\n",
    "print(\"   ...\")\n",
    "sample['parcelid'] = sample['ParcelId']\n",
    "print(\"   Merge with property data ...\")\n",
    "df_test = sample.merge(prop, on='parcelid', how='left')\n",
    "print(\"   ...\")\n",
    "del sample, prop; gc.collect()\n",
    "print(\"   ...\")\n",
    "x_test = df_test[train_columns]\n",
    "print(\"   ...\")\n",
    "del df_test; gc.collect()\n",
    "print(\"   Preparing x_test...\")\n",
    "for c in x_test.dtypes[x_test.dtypes == object].index.values:\n",
    "    x_test[c] = (x_test[c] == True)\n",
    "print(\"   ...\")\n",
    "x_test = x_test.values.astype(np.float32, copy=False)\n",
    "\n",
    "print(\"\\nStart LightGBM prediction ...\")\n",
    "# num_threads > 1 will predict very slow in kernal\n",
    "clf.reset_parameter({\"num_threads\":1})\n",
    "p_test = clf.predict(x_test)\n",
    "\n",
    "del x_test; gc.collect()\n",
    "\n",
    "print( \"\\nUnadjusted LightGBM predictions:\" )\n",
    "print( pd.DataFrame(p_test).head() )\n",
    "\n",
    "\n",
    "\n",
    "##### RE-READ PROPERTIES FILE\n",
    "##### (I tried keeping a copy, but the program crashed.)\n",
    "\n",
    "print( \"\\nRe-reading properties file ...\")\n",
    "properties = pd.read_csv('../input/properties_2016.csv')\n",
    "\n",
    "\n",
    "\n",
    "##### PROCESS DATA FOR XGBOOST\n",
    "\n",
    "print( \"\\nProcessing data for XGBoost ...\")\n",
    "for c in properties.columns:\n",
    "    properties[c]=properties[c].fillna(-1)\n",
    "    if properties[c].dtype == 'object':\n",
    "        lbl = LabelEncoder()\n",
    "        lbl.fit(list(properties[c].values))\n",
    "        properties[c] = lbl.transform(list(properties[c].values))\n",
    "\n",
    "train_df = train.merge(properties, how='left', on='parcelid')\n",
    "x_train = train_df.drop(['parcelid', 'logerror','transactiondate'], axis=1)\n",
    "x_test = properties.drop(['parcelid'], axis=1)\n",
    "# shape        \n",
    "print('Shape train: {}\\nShape test: {}'.format(x_train.shape, x_test.shape))\n",
    "\n",
    "# drop out ouliers\n",
    "train_df=train_df[ train_df.logerror > -0.4 ]\n",
    "train_df=train_df[ train_df.logerror < 0.418 ]\n",
    "x_train=train_df.drop(['parcelid', 'logerror','transactiondate'], axis=1)\n",
    "y_train = train_df[\"logerror\"].values.astype(np.float32)\n",
    "y_mean = np.mean(y_train)\n",
    "\n",
    "print('After removing outliers:')     \n",
    "print('Shape train: {}\\nShape test: {}'.format(x_train.shape, x_test.shape))\n",
    "\n",
    "\n",
    "\n",
    "##### RUN XGBOOST\n",
    "\n",
    "print(\"\\nSetting up data for XGBoost ...\")\n",
    "# xgboost params\n",
    "xgb_params = {\n",
    "    'eta': 0.037,\n",
    "    'max_depth': 5,\n",
    "    'subsample': 0.80,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'mae',\n",
    "    'lambda': 0.8,   \n",
    "    'alpha': 0.4, \n",
    "    'base_score': y_mean,\n",
    "    'silent': 1\n",
    "}\n",
    "# Enough with the ridiculously overfit parameters.\n",
    "# I'm going back to my version 20 instead of copying Jayaraman.\n",
    "# I want a num_boost_rounds that's chosen by my CV,\n",
    "# not one that's chosen by overfitting the public leaderboard.\n",
    "# (There may be underlying differences between the train and test data\n",
    "#  that will affect some parameters, but they shouldn't affect that.)\n",
    "\n",
    "dtrain = xgb.DMatrix(x_train, y_train)\n",
    "dtest = xgb.DMatrix(x_test)\n",
    "\n",
    "# cross-validation\n",
    "#print( \"Running XGBoost CV ...\" )\n",
    "#cv_result = xgb.cv(xgb_params, \n",
    "#                   dtrain, \n",
    "#                   nfold=5,\n",
    "#                   num_boost_round=350,\n",
    "#                   early_stopping_rounds=50,\n",
    "#                   verbose_eval=10, \n",
    "#                   show_stdv=False\n",
    "#                  )\n",
    "#num_boost_rounds = len(cv_result)\n",
    "\n",
    "# num_boost_rounds = 150\n",
    "num_boost_rounds = 242\n",
    "print(\"\\nXGBoost tuned with CV in:\")\n",
    "print(\"   https://www.kaggle.com/aharless/xgboost-without-outliers-tweak \")\n",
    "print(\"num_boost_rounds=\"+str(num_boost_rounds))\n",
    "\n",
    "# train model\n",
    "print( \"\\nTraining XGBoost ...\")\n",
    "model = xgb.train(dict(xgb_params, silent=1), dtrain, num_boost_round=num_boost_rounds)\n",
    "\n",
    "print( \"\\nPredicting with XGBoost ...\")\n",
    "xgb_pred = model.predict(dtest)\n",
    "\n",
    "print( \"\\nXGBoost predictions:\" )\n",
    "print( pd.DataFrame(xgb_pred).head() )\n",
    "\n",
    "\n",
    "\n",
    "##### COMBINE PREDICTIONS\n",
    "\n",
    "print( \"\\nCombining XGBoost, LightGBM, and baseline predicitons ...\" )\n",
    "lgb_weight = 1 - XGB_WEIGHT - BASELINE_WEIGHT\n",
    "pred = XGB_WEIGHT*xgb_pred + BASELINE_WEIGHT*BASELINE_PRED + lgb_weight*p_test\n",
    "\n",
    "print( \"\\nCombined predictions:\" )\n",
    "print( pd.DataFrame(pred).head() )\n",
    "\n",
    "\n",
    "\n",
    "##### WRITE THE RESULTS\n",
    "\n",
    "print( \"\\nPreparing results for write ...\" )\n",
    "y_pred=[]\n",
    "\n",
    "for i,predict in enumerate(pred):\n",
    "    y_pred.append(str(round(predict,4)))\n",
    "y_pred=np.array(y_pred)\n",
    "\n",
    "output = pd.DataFrame({'ParcelId': properties['parcelid'].astype(np.int32),\n",
    "        '201610': y_pred, '201611': y_pred, '201612': y_pred,\n",
    "        '201710': y_pred, '201711': y_pred, '201712': y_pred})\n",
    "# set col 'ParceID' to first col\n",
    "cols = output.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "output = output[cols]\n",
    "from datetime import datetime\n",
    "\n",
    "print( \"\\nWriting results to disk ...\" )\n",
    "output.to_csv('sub{}.csv'.format(datetime.now().strftime('%Y%m%d_%H%M%S')), index=False)\n",
    "\n",
    "print( \"\\nFinished ...\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
